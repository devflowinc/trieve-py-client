# coding: utf-8

"""
    Trieve API

    Trieve OpenAPI Specification. This document describes all of the operations available through the Trieve API.

    The version of the OpenAPI document: 0.5.7
    Contact: developers@trieve.ai
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictBool, StrictStr
from typing import Any, ClassVar, Dict, List, Optional
from trieve_py_client.models.chat_message_proxy import ChatMessageProxy
from typing import Optional, Set
from typing_extensions import Self

class GenerateChunksRequest(BaseModel):
    """
    GenerateChunksRequest
    """ # noqa: E501
    chunk_ids: List[StrictStr] = Field(description="The ids of the chunks to be retrieved and injected into the context window for RAG.")
    model: Optional[StrictStr] = Field(default=None, description="The model to use for the chat. This can be any model from the openrouter model list. If no model is provided, gpt-3.5-turbo will be used.")
    prev_messages: List[ChatMessageProxy] = Field(description="The previous messages to be placed into the chat history. The last message in this array will be the prompt for the model to inference on. The length of this array must be at least 1.")
    prompt: Optional[StrictStr] = Field(default=None, description="Prompt for the last message in the prev_messages array. This will be used to generate the next message in the chat. The default is 'Respond to the instruction and include the doc numbers that you used in square brackets at the end of the sentences that you used the docs for:'. You can also specify an empty string to leave the final message alone such that your user's final message can be used as the prompt. See docs.trieve.ai or contact us for more information.")
    stream_response: Optional[StrictBool] = Field(default=None, description="Whether or not to stream the response. If this is set to true or not included, the response will be a stream. If this is set to false, the response will be a normal JSON response. Default is true.")
    __properties: ClassVar[List[str]] = ["chunk_ids", "model", "prev_messages", "prompt", "stream_response"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of GenerateChunksRequest from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of each item in prev_messages (list)
        _items = []
        if self.prev_messages:
            for _item in self.prev_messages:
                if _item:
                    _items.append(_item.to_dict())
            _dict['prev_messages'] = _items
        # set to None if model (nullable) is None
        # and model_fields_set contains the field
        if self.model is None and "model" in self.model_fields_set:
            _dict['model'] = None

        # set to None if prompt (nullable) is None
        # and model_fields_set contains the field
        if self.prompt is None and "prompt" in self.model_fields_set:
            _dict['prompt'] = None

        # set to None if stream_response (nullable) is None
        # and model_fields_set contains the field
        if self.stream_response is None and "stream_response" in self.model_fields_set:
            _dict['stream_response'] = None

        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of GenerateChunksRequest from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "chunk_ids": obj.get("chunk_ids"),
            "model": obj.get("model"),
            "prev_messages": [ChatMessageProxy.from_dict(_item) for _item in obj["prev_messages"]] if obj.get("prev_messages") is not None else None,
            "prompt": obj.get("prompt"),
            "stream_response": obj.get("stream_response")
        })
        return _obj


